{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build, Train and Deploy an XgBoost model on SageMaker\n",
    "\n",
    "In this notebook, we will walk through how to process data, train a model and deploy a trained model endpoint. \n",
    "\n",
    "We will also demonstrate, how to use SageMaker Processing to build a custom data processing pipeline, and use SageMaker Experiments to track the lineage of the trained models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker-experiments xgboost -Uqq\n",
    "!conda install -c conda-forge -q -y shap > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the role we have created for our notebook here:\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.session.Session().region_name\n",
    "print(\"Region = {}\".format(region))\n",
    "sm = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import other libraries such as sagemaker experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import sleep, gmtime, strftime\n",
    "import json\n",
    "import time\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SageMaker Experiments \n",
    "\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify buckets for storing data\n",
    "\n",
    "Here you specify the S3 bucket and file paths where a number of artifacts will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our custom bucket here. \n",
    "# Modify the bucket to grant acess to your bucket\n",
    "rawbucket= sess.default_bucket() \n",
    "prefix = 'sagemaker-modelmonitor-cc-default' # use this prefix to store all files pertaining to this workshop.\n",
    "\n",
    "dataprefix = prefix + '/data'\n",
    "traindataprefix = prefix + '/train_data'\n",
    "testdataprefix = prefix + '/test_data'\n",
    "testdatanolabelprefix = prefix + '/test_data_no_label'\n",
    "trainheaderprefix = prefix + '/train_headers'\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key part of the data science lifecyle is data exploration, pre-processing and feature engineering. We will demonstrate how to use SM notebooks for data exploration and SM Processing for feature engineering and pre-processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/stefannatu/ModelMonitor/raw/master/credit_card_default_data.xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('./credit_card_default_data.xls', header=1)\n",
    "data = data.drop(columns = ['ID'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the categorical columns SEX, Education and Marriage have been Integer Encoded in this case.\n",
    "# For example:\n",
    "data.SEX.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={\"default payment next month\": \"Label\"}, inplace=True)\n",
    "lbl = data.Label\n",
    "data = pd.concat([lbl, data.drop(columns=['Label'])], axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(data.Label)\n",
    "plt.title('Counts of Default versus Non Default Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Corr plot\n",
    "f = plt.figure(figsize=(19, 15))\n",
    "plt.matshow(data.corr(), fignum=f.number)\n",
    "plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=45)\n",
    "plt.yticks(range(data.shape[1]), data.columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "SCAT_COLUMNS = ['BILL_AMT1', 'BILL_AMT2', 'PAY_AMT1', 'PAY_AMT2']\n",
    "scatter_matrix(data[SCAT_COLUMNS],figsize=(10, 10), diagonal ='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Feature Engineering in Notebook\n",
    "\n",
    "For small datasets or testing out your ETL script, you can run pre-processing inside a notebook directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('rawdata/rawdata.csv'):\n",
    "    !mkdir ./rawdata\n",
    "    data.to_csv('rawdata/rawdata.csv', index=None)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the raw dataset\n",
    "raw_data_location = sess.upload_data('rawdata', bucket=rawbucket, key_prefix=dataprefix)\n",
    "print(raw_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the preprocessing job in the notebook and upload the training and validation datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "COLS = data.columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('Label', axis=1), data['Label'], \n",
    "                                                       test_size=0.2, random_state=0)\n",
    "\n",
    "newcolorder = ['PAY_AMT1','BILL_AMT1'] + list(COLS[1:])[:11] + list(COLS[1:])[12:17] + list(COLS[1:])[18:]\n",
    "\n",
    "preprocess = make_column_transformer(\n",
    "        (StandardScaler(), ['PAY_AMT1']),\n",
    "        (MinMaxScaler(),['BILL_AMT1']),\n",
    "    remainder='passthrough')\n",
    "    \n",
    "print('Running preprocessing and feature engineering transformations')\n",
    "train_features = pd.DataFrame(preprocess.fit_transform(X_train), columns = newcolorder)\n",
    "test_features = pd.DataFrame(preprocess.transform(X_test), columns = newcolorder)\n",
    "\n",
    "train_full = pd.concat([pd.DataFrame(y_train.values, columns=['Label']), pd.DataFrame(train_features)], axis=1)\n",
    "test_full = pd.concat([pd.DataFrame(y_test.values, columns=['Label']), pd.DataFrame(test_features)], axis=1)\n",
    "train_full.to_csv('train_data.csv', index=False, header=False)\n",
    "test_full.to_csv('test_data.csv', index=False, header=False)                                                   \n",
    "train_full.to_csv('train_data_with_headers.csv', index=False)                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "train_data_location = sess.upload_data('train_data_with_headers.csv', bucket=rawbucket, key_prefix=trainheaderprefix)\n",
    "train_data_location = sess.upload_data('train_data.csv', bucket=rawbucket, key_prefix=traindataprefix)\n",
    "test_data_location = sess.upload_data('test_data.csv', bucket=rawbucket, key_prefix=testdataprefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secure Feature Processing pipeline using SageMaker Processing\n",
    "\n",
    "While you can pre-process small amounts of data directly in a notebook as shown above, SageMaker Processing offloads the heavy lifting of pre-processing larger datasets by provisioning the underlying infrastructure, downloading the data from an S3 location to the processing container, running the processing scripts, storing the processed data in an output directory in Amazon S3 and deleting the underlying transient resources needed to run the processing job. Once the processing job is complete, the infrastructure used to run the job is wiped, and any temporary data stored on it is deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use SageMaker Processing with Sklearn. -- combine data into train and test at this stage if possible.\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.c4.xlarge',\n",
    "                                     instance_count=1\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a preprocessing script (same as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-test-split-ratio', type=float, default=0.3)\n",
    "    parser.add_argument('--random-split', type=int, default=0)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print('Received arguments {}'.format(args))\n",
    "\n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', 'rawdata.csv')\n",
    "    \n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    df.sample(frac=1)\n",
    "    \n",
    "    COLS = df.columns\n",
    "    newcolorder = ['PAY_AMT1','BILL_AMT1'] + list(COLS[1:])[:11] + list(COLS[1:])[12:17] + list(COLS[1:])[18:]\n",
    "    \n",
    "    split_ratio = args.train_test_split_ratio\n",
    "    random_state=args.random_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('Label', axis=1), df['Label'], \n",
    "                                                        test_size=split_ratio, random_state=random_state)\n",
    "    \n",
    "    preprocess = make_column_transformer(\n",
    "        (['PAY_AMT1'], StandardScaler()),\n",
    "        (['BILL_AMT1'], MinMaxScaler()),\n",
    "    remainder='passthrough')\n",
    "    \n",
    "    print('Running preprocessing and feature engineering transformations')\n",
    "    train_features = pd.DataFrame(preprocess.fit_transform(X_train), columns = newcolorder)\n",
    "    test_features = pd.DataFrame(preprocess.transform(X_test), columns = newcolorder)\n",
    "    \n",
    "    # concat to ensure Label column is the first column in dataframe\n",
    "    train_full = pd.concat([pd.DataFrame(y_train.values, columns=['Label']), train_features], axis=1)\n",
    "    test_full = pd.concat([pd.DataFrame(y_test.values, columns=['Label']), test_features], axis=1)\n",
    "    \n",
    "    print('Train data shape after preprocessing: {}'.format(train_features.shape))\n",
    "    print('Test data shape after preprocessing: {}'.format(test_features.shape))\n",
    "    \n",
    "    train_features_headers_output_path = os.path.join('/opt/ml/processing/train_headers', 'train_data_with_headers.csv')\n",
    "    \n",
    "    train_features_output_path = os.path.join('/opt/ml/processing/train', 'train_data.csv')\n",
    "    \n",
    "    test_features_output_path = os.path.join('/opt/ml/processing/test', 'test_data.csv')\n",
    "    \n",
    "    print('Saving training features to {}'.format(train_features_output_path))\n",
    "    train_full.to_csv(train_features_output_path, header=False, index=False)\n",
    "    print(\"Complete\")\n",
    "    \n",
    "    print(\"Save training data with headers to {}\".format(train_features_headers_output_path))\n",
    "    train_full.to_csv(train_features_headers_output_path, index=False)\n",
    "                 \n",
    "    print('Saving test features to {}'.format(test_features_output_path))\n",
    "    test_full.to_csv(test_features_output_path, header=False, index=False)\n",
    "    print(\"Complete\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the preprocessing code over to the s3 bucket\n",
    "codeprefix = prefix + '/code'\n",
    "codeupload = sess.upload_data('preprocessing.py', bucket=rawbucket, key_prefix=codeprefix)\n",
    "print(codeupload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_location = rawbucket + '/' + traindataprefix\n",
    "test_data_location = rawbucket+'/'+testdataprefix\n",
    "print(\"Training data location = {}\".format(train_data_location))\n",
    "print(\"Test data location = {}\".format(test_data_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(code=codeupload,\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=raw_data_location,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/train',\n",
    "                               destination='s3://' + train_data_location),\n",
    "                               ProcessingOutput(output_name='test_data',\n",
    "                                                source='/opt/ml/processing/test',\n",
    "                                               destination=\"s3://\"+test_data_location),\n",
    "                               ProcessingOutput(output_name='train_data_headers',\n",
    "                                                source='/opt/ml/processing/train_headers',\n",
    "                                               destination=\"s3://\" + rawbucket + '/' + prefix + '/train_headers')],\n",
    "                      arguments=['--train-test-split-ratio', '0.2'] # specify any arguments needed here\n",
    "                     )\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train_data':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'test_data':\n",
    "        preprocessed_test_data = output['S3Output']['S3Uri']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model development and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traceability and Auditability \n",
    "\n",
    "We use SageMaker Experiments for data scientists to track the lineage of the model from the raw data source to the preprocessing steps and the model training pipeline. With SageMaker Experiments, data scientists can compare, track and manage multiple diferent model training jobs, data processing jobs, hyperparameter tuning jobs and retain a lineage from the source data to the training job artifacts to the model hyperparameters and any custom metrics that they may want to monitor as part of the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SageMaker Experiment\n",
    "cc_experiment = Experiment.create(\n",
    "    experiment_name=f\"Sagemaker-Workshop-Morgan-{int(time.time())}\", \n",
    "    description=\"Predict credit card default from payments data\", \n",
    "    sagemaker_boto_client=sm)\n",
    "print(cc_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Tracking parameters used in the Pre-processing pipeline.\n",
    "with Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n",
    "    tracker.log_parameters({\n",
    "        \"train_test_split_ratio\": 0.2,\n",
    "        \"random_state\":0\n",
    "    })\n",
    "    # we can log the s3 uri to the dataset we just uploaded\n",
    "    tracker.log_input(name=\"ccdefault-raw-dataset\", media_type=\"s3/uri\", value=raw_data_location)\n",
    "    tracker.log_input(name=\"ccdefault-train-dataset\", media_type=\"s3/uri\", value=train_data_location)\n",
    "    tracker.log_input(name=\"ccdefault-test-dataset\", media_type=\"s3/uri\", value=test_data_location)  \n",
    "preprocessing_trial_component = tracker.trial_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "The same security postures we applied previously during SM Processing apply to training jobs. We will also have SageMaker experiments track the training job and store metadata such as model artifact location, training/validation data location, model hyperparameters etc.\n",
    "\n",
    "Now you will kick off an XGBoost model training job directly on the processed data. You will use the Built in SageMaker algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the container image from th ebjl\n",
    "image = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\") #path to your ECR docker file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.session.s3_input(s3_data=f's3://{rawbucket}/{prefix}/train_data/train_data.csv', content_type='csv')\n",
    "inputs = {'train': s3_input_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify some parameters for SageMaker Experiments to track your Experiments\n",
    "\n",
    "trial_name = f\"cc-fraud-training-job-{int(time.time())}\"\n",
    "cc_trial = Trial.create(\n",
    "        trial_name=trial_name, \n",
    "            experiment_name=cc_experiment.experiment_name,\n",
    "        sagemaker_boto_client=sm\n",
    "    )\n",
    "\n",
    "cc_trial.add_trial_component(preprocessing_trial_component)\n",
    "cc_training_job_name = \"cc-training-job-{}\".format(int(time.time()))\n",
    "\n",
    "experiment_config={\n",
    "            \"TrialName\": cc_trial.trial_name, #log training job in Trials for lineage\n",
    "            \"TrialComponentDisplayName\": \"Train\", #\"Training\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"100\",\n",
    "    \"eval_metric\": \"auc\"\n",
    "}\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(image,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/models'.format(rawbucket, prefix),\n",
    "                                    hyperparameters=hyperparameters,\n",
    "                                    sagemaker_session=sess,\n",
    "                                    disable_profiler=True) # set to true for distributed training\n",
    "\n",
    "xgb.fit(inputs = inputs, job_name=cc_training_job_name, experiment_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5. Traceability and Auditability from source control to Model artifacts\n",
    "\n",
    "Having used SageMaker Experiments to track the training runs, we can now extract model metadata to get the entire lineage of the model from the source data to the model artifacts and the hyperparameters.\n",
    "\n",
    "To do this, simply call the **describe_trial_component** API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Present the Model Lineage as a dataframe\n",
    "from sagemaker.session import Session\n",
    "session = boto3.Session()\n",
    "lineage_table = ExperimentAnalytics(\n",
    "    sagemaker_session=Session(session, sm), \n",
    "    search_expression={\n",
    "        \"Filters\":[{\n",
    "            \"Name\": \"Parents.TrialName\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": trial_name\n",
    "        }]\n",
    "    },\n",
    "    sort_by=\"CreationTime\",\n",
    "    sort_order=\"Ascending\",\n",
    ")\n",
    "lineagedf= lineage_table.dataframe()\n",
    "\n",
    "lineagedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get detailed information about a particular trial\n",
    "sm.describe_trial_component(TrialComponentName=lineagedf.TrialComponentName[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model Explainability\n",
    "\n",
    "Model artifacts generated by a SageMaker Training job can be easily downloaded for further analysis. In this section, we'll use the [shap](https://github.com/slundberg/shap) python library to explain the output of the machine learning model that we've trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model artifact from S3\n",
    "import tarfile\n",
    "import pickle\n",
    "print(f\"Downloading model artifact from {xgb.model_data}\")\n",
    "with s3.open(xgb.model_data, \"rb\") as f:\n",
    "    tar = tarfile.open(fileobj=f, mode='r:gz')\n",
    "    for item in tar:\n",
    "        model = pickle.load(tar.extractfile(item.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "from xgboost import DMatrix\n",
    "\n",
    "model.feature_names = test_full.columns[1:]\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "shap_values = explainer.shap_values(train_full.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__MODEL FEATURES__\n",
    "- LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit)\n",
    "- SEX: Gender (1=male, 2=female)\n",
    "- EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others)\n",
    "- MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
    "- AGE: Age in years\n",
    "- PAY_0: Repayment status in September, 2005 (0=pay duly, 1=payment delay for one month, 2=payment delay for two months, â€¦ 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
    "- PAY_2: Repayment status in August, 2005 (scale same as above)\n",
    "- PAY_3: Repayment status in July, 2005 (scale same as above)\n",
    "- PAY_4: Repayment status in June, 2005 (scale same as above)\n",
    "- PAY_5: Repayment status in May, 2005 (scale same as above)\n",
    "- PAY_6: Repayment status in April, 2005 (scale same as above)\n",
    "- BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
    "- BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
    "- BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
    "- BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
    "- BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
    "- BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
    "- PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
    "- PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
    "- PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
    "- PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
    "- PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
    "- PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary plot to get the most impactful features\n",
    "shap.summary_plot(shap_values, train_full.iloc[:,1:], plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary plot showing the marginal contribution of each feature to every prediction \n",
    "shap.summary_plot(shap_values, train_full.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depence plot with an automactially slected interaction feature\n",
    "shap.dependence_plot(\"SEX\", shap_values, train_full.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain inidvidual prediction\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], train_full.iloc[0,1:], link=\"logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "\n",
    "Now you will deploy the model to a hosted endpoint as well as run offline inference using SageMaker Batch Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform\n",
    "\n",
    "Let's first use Batch Transform to run through inference for the test dataset. This is used for offline inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_no_label = test_full.drop(columns = ['Label'], axis=1)\n",
    "label = test_full['Label']\n",
    "test_data_no_label.to_csv('test_data_no_label.csv', index=False, header=False)\n",
    "test_data_no_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_nohead_location = sess.upload_data('test_data_no_label.csv', bucket=rawbucket, key_prefix=testdatanolabelprefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run Batch Transform -- you simply need to call the Transfomer API below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sm_transformer = xgb.transformer(1, 'ml.m5.xlarge', accept = 'text/csv')\n",
    "\n",
    "# start a transform job\n",
    "sm_transformer.transform(test_data_nohead_location, split_type='Line', content_type='text/csv',\n",
    "                        experiment_config={\n",
    "            \"TrialName\": cc_trial.trial_name, #log training job in Trials for lineage\n",
    "            \"TrialComponentDisplayName\": \"Batch-Transform\", #\"Training\",\n",
    "        })\n",
    "sm_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_csv_output_from_s3(s3uri, file_name):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket_name = parsed_url.netloc\n",
    "    prefix = parsed_url.path[1:]\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Object(bucket_name, '{}/{}'.format(prefix, file_name))\n",
    "    return obj.get()[\"Body\"].read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_csv_output_from_s3(sm_transformer.output_path, 'test_data_no_label.csv.out')\n",
    "output_df = pd.read_csv(io.StringIO(output), sep=\",\", header=None)\n",
    "output_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df['result'] = np.round(output_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline Accuracy = {}\".format(1- np.unique(data['Label'], return_counts=True)[1][1]/(len(data['Label']))))\n",
    "print(\"Accuracy Score = {}\".format(accuracy_score(label, output_df['result'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df['Predicted']=output_df['result'].values\n",
    "output_df['Label'] = label\n",
    "confusion_matrix = pd.crosstab(output_df['Predicted'], output_df['Label'], rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment as Hosted Endpoint\n",
    "\n",
    "Now we will deploy the model as a hosted endpoint. Again this is a single line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Model Monitor API\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.predictor import csv_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker Model object\n",
    "\n",
    "SageMaker model will package your trained artifacts into a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "latest_training_job = sm_client.list_training_jobs(\n",
    "    MaxResults=1,\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending')\n",
    "\n",
    "training_job_name = TrainingJobName = latest_training_job['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "\n",
    "training_job_description = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "\n",
    "model_data = training_job_description['ModelArtifacts']['S3ModelArtifacts']\n",
    "container_uri = training_job_description['AlgorithmSpecification']['TrainingImage']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = sagemaker.model.Model(image_uri=container_uri,\n",
    "                                 model_data = model_data,\n",
    "                                 role =role,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_capture_upload_path = 's3://{}/{}/monitoring/datacapture'.format(rawbucket, prefix)\n",
    "\n",
    "data_capture_configuration = DataCaptureConfig(enable_capture =True,                                          \n",
    "                                               sampling_percentage=100,\n",
    "                                               destination_s3_uri=s3_capture_upload_path,\n",
    "                                             capture_options=[\"Input\", \"Output\"],\n",
    "                                            csv_content_types= [\"text/csv\"],\n",
    "                                            json_content_types= [\"application/json\"]\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy!\n",
    "\n",
    "You can deploy your model with a single line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.deploy(1, 'ml.m4.xlarge', data_capture_config=data_capture_configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the model deployed properly.\n",
    "\n",
    "Replace \"endpoint_name\" with the name of your endpoint above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker-runtime')\n",
    "endpoint_name = sm_client.list_endpoints(\n",
    "    MaxResults=1,\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending')['Endpoints'][0]['EndpointName']\n",
    "print(endpoint_name)\n",
    "\n",
    "!head -10 test_data.csv > test_sample.csv\n",
    "\n",
    "with open('test_sample.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = client.invoke_endpoint(\n",
    "        EndpointName= endpoint_name,\n",
    "        Body= payload[2:],\n",
    "        ContentType = 'text/csv')\n",
    "        sleep(0.5)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the captured json files.\n",
    "data_capture_prefix = '{}/monitoring/datacapture'.format(prefix)\n",
    "s3_client = boto3.Session().client('s3')\n",
    "current_endpoint_capture_prefix = '{}/{}/AllTraffic'.format(data_capture_prefix, endpoint_name)\n",
    "result = s3_client.list_objects(Bucket=rawbucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))\n",
    "\n",
    "capture_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you get an error above --it takes some time for the data to be captured into the S3 bucket. Rerun this cell above again in a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View contents of the captured file.\n",
    "def get_obj_body(bucket, obj_key):\n",
    "    return s3_client.get_object(Bucket=rawbucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(rawbucket, capture_files[-1])\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[5]), indent = 2, sort_keys =True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=f\"s3://{rawbucket}/{prefix}/train_data/train_data.csv\",\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=f\"s3://{rawbucket}/{prefix}/baseline_mm/\",\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Underlying Resources\n",
    "\n",
    "Make sure to delete your endpoint once you are done. Other resources such as instances for training and processing are automatically deleted by SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName = endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
