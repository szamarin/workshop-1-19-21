{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries Forecasting with Prophet\n",
    "In this notebook we'll go through an end to end example of training and deploying a timeseries model using the fbprophet libarary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install pystan==2.18\n",
    "!pip install fbprophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO, StringIO\n",
    "import pandas as pd\n",
    "from itertools import starmap\n",
    "from datetime import datetime\n",
    "import sagemaker\n",
    "\n",
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup sagemaker variables\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket()\n",
    "key_prefix = \"air-quality-prophet/{}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Beijing Multi-Site Air-Quality Data Data Set hosted by UCI. The data set includes hourly air pollutants data from 12 nationally-controlled air-quality monitoring sites. For this example we'll take data from a single site and build a model to predict the amount of PM2.5 (fine particulates) in the air on a given day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download air quality time-series data from UCI\n",
    "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00501/PRSA2017_Data_20130301-20170228.zip\"\n",
    "zip_file = ZipFile(BytesIO(requests.get(data_url).content))\n",
    "data = StringIO(zip_file.read(\"PRSA_Data_20130301-20170228/PRSA_Data_Changping_20130301-20170228.csv\").decode(\"utf8\"))\n",
    "\n",
    "df = pd.read_csv(data)\n",
    "# convert to datetime index\n",
    "df.index = df.apply(lambda x: datetime(x.year, x.month, x.day, x.hour), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a quick example locally to see how easy it is to train a timeseries model with fbprophet. The official Prophet site provides **[documentation](https://facebook.github.io/prophet/docs/quick_start.html#python-api)** for fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to daily averages; split into train/test sets and only include the PM2.5 variables\n",
    "train = df.loc[df[\"year\"] < 2016, \"PM2.5\"].resample(\"1D\").mean().reset_index().rename(columns={\"index\":\"ds\", \"PM2.5\":\"y\"})\n",
    "test = df.loc[df[\"year\"] >= 2016, \"PM2.5\"].resample(\"1D\").mean().reset_index().rename(columns={\"index\":\"ds\", \"PM2.5\":\"y\"})\n",
    "\n",
    "train.fillna(method=\"ffill\", inplace=True)\n",
    "test.fillna(method=\"ffill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train locally\n",
    "m = Prophet()\n",
    "m.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions and compare against test data\n",
    "future = m.make_future_dataframe(periods=test.shape[0], include_history=False)\n",
    "forecast = m.predict(future)\n",
    "\n",
    "ax = test.set_index(\"ds\").plot(figsize = (14,6))\n",
    "forecast[[\"ds\", \"yhat\"]].set_index(\"ds\").plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Remotely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model remotely we need to do several things:\n",
    "1. Upload our data to S3\n",
    "2. Convert the notebook code above into a training script\n",
    "3. Create a requirements.txt file to include additional dependencies (e.g. fbprophet) that are not present in the SKLearn container\n",
    "4. Use the SageMaker SDK to configure and run a managed training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data to S3\n",
    "for name, data_set in zip([\"train\",\"test\"],[train, test]):\n",
    "    sess.upload_string_as_file_body(body=data_set.to_csv(index=False),\n",
    "                                   bucket=bucket,\n",
    "                                   key=key_prefix.format(f\"{name}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the notebook traing code above needs to be converted into a training script\n",
    "# conviniently this training job will also generate predictions and save the results to S3\n",
    "!pygmentize src/train.py | cat -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# setup a SageMaker training job (review tarin.py script in src directory)\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "prophet_estimator = SKLearn(source_dir=\"src\",\n",
    "                            entry_point=\"train.py\",\n",
    "                            framework_version=\"0.23-1\",\n",
    "                            hyperparameters = {\"changepoint_prior_scale\": 0.005, \n",
    "                                               \"prediction_periods\": 425},\n",
    "                            train_instance_type=\"local\", # will spin up a training container on the local notebook instance instead of launching a new virtual machine.\n",
    "                            role=role)\n",
    "\n",
    "train_data_path = f\"s3://{bucket}/{key_prefix.format('train.csv')}\"\n",
    "\n",
    "prophet_estimator.fit({\"train\":train_data_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serving a timeseries model is not quite the same as serving an xgboost model. In XGBoost and SKLearn examples, we sent or model a record and it returned a prediction (i.e. a one to one mapping between input records and predictions returned). With timeseries forecasting though this may not be the case and depending on the usecase we may have any number of ways of serving predictions such as:\n",
    "- **Input**: a list of dates; **Output**: forecast for the dates provided\n",
    "- **Input**: starting date and number of periods forward; **Output**: Forward forecast for the period specified\n",
    "- **Input**: start and end date; **Output**: forecast for each date between start and end\n",
    "With SageMaker you can pass in a serving script to customize how inputs tor your model are processed and how the outputs of your model are post-processed before returning the results to the user. In this example we'll provide a serving script that will take a json input like this:\n",
    "```\n",
    "    {\n",
    "        \"start\": \"1/1/2016\",\n",
    "        \"end\": \"2/28/2017\"\n",
    "    }\n",
    "```\n",
    "And will return a list of predictions for each date that falls within the specified start-end period:\n",
    "```\n",
    "    [\n",
    "        {\"ds\":1/1/2016, \"yhat_lower\": 5,\"yhat_upper\": 20, \"yhat\": 12},\n",
    "        ...\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a serving script to host the model as a rest api endpoint\n",
    "!pygmentize src/serve.py | cat -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# deploy as a REST Endpoint \n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "prophet_model = SKLearnModel(source_dir=\"src\",\n",
    "                         entry_point=\"serve.py\",\n",
    "                         framework_version=\"0.23-1\",\n",
    "                         model_data = prophet_estimator.model_data,\n",
    "                         role = role\n",
    "                    )\n",
    "\n",
    "predictor = prophet_model.deploy(instance_type=\"local\", \n",
    "                initial_instance_count=1)\n",
    "\n",
    "predictor.serializer = json_serializer # convert python dict input to json \n",
    "predictor.deserializer = json_deserializer # converts binary encoded json string output to python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's ge predictions from our newly deployed endpoint\n",
    "predictions = predictor.predict({\"start\": \"1-1-2016\", \"end\": \"2-28-2017\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions to a dataframe and visualize\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df[\"ds\"] = pd.to_datetime(predictions_df[\"ds\"])\n",
    "\n",
    "ax = test.set_index(\"ds\").plot(figsize = (14,6))\n",
    "predictions_df[[\"ds\", \"yhat\"]].set_index(\"ds\").plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally SageMaker provides a HyperparameterTuner which allows you to run multiple concurrent jobs to find the best set pf hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import CategoricalParameter, ContinuousParameter, HyperparameterTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a base training job\n",
    "prophet_estimator_hpo = SKLearn(source_dir=\"src\",\n",
    "                            entry_point=\"train-hpo.py\",\n",
    "                            framework_version=\"0.23-1\",\n",
    "                            train_instance_type=\"ml.m5.large\",\n",
    "                            role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide hyperparam ranges for grid search\n",
    "hyperparameter_ranges = {'changepoint_prior_scale': ContinuousParameter(0.001, 0.5),\n",
    "                        'seasonality_prior_scale': ContinuousParameter(0.01, 5),\n",
    "                        'seasonality_mode': CategoricalParameter(['additive', 'multiplicative'])\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and traom a HyperparameterTuner object\n",
    "tuner = HyperparameterTuner(estimator=prophet_estimator_hpo, \n",
    "                            objective_metric_name=\"rmse\", \n",
    "                            metric_definitions=[{\"Name\":\"rmse\", \"Regex\":\"train rmse: (\\S+)\"}], # SageMaker will parse the target metric from the logs\n",
    "                            hyperparameter_ranges=hyperparameter_ranges,\n",
    "                            max_jobs=20, \n",
    "                            max_parallel_jobs=5,\n",
    "                            objective_type='Minimize'\n",
    "                       )\n",
    "\n",
    "tuner.fit({\"train\":train_data_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view details of tuning job\n",
    "sess.describe_tuning_job(tuner.latest_tuning_job.job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the best model and deploy it\n",
    "best_job = sess.describe_tuning_job(tuner.latest_tuning_job.job_name)[\"BestTrainingJob\"][\"TrainingJobName\"]\n",
    "best_model = sess.describe_training_job(best_job)['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "prophet_model = SKLearnModel(source_dir=\"src\",\n",
    "                         entry_point=\"serve.py\",\n",
    "                         framework_version=\"0.23-1\",\n",
    "                         model_data = best_model,\n",
    "                         role = role\n",
    "                    )\n",
    "\n",
    "predictor = prophet_model.deploy(instance_type=\"local\", \n",
    "                initial_instance_count=1)\n",
    "\n",
    "predictor.serializer = json_serializer # convert python dict input to json \n",
    "predictor.deserializer = json_deserializer # converts binary encoded json string output to python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictor.predict({\"start\": \"1-1-2016\", \"end\": \"2-28-2017\"})\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df[\"ds\"] = pd.to_datetime(predictions_df[\"ds\"])\n",
    "ax = test.set_index(\"ds\").plot(figsize = (14,6))\n",
    "predictions_df[[\"ds\", \"yhat\"]].set_index(\"ds\").plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python2",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
